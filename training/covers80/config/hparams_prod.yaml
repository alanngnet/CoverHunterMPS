### CoverHunterMPS production training hyperparameters

### Training data paths
train_path: "data/covers80/full.txt"
test_path: "data/reels50easy_testset/full.txt"
# Note that val_path is ignored for train_prod because validation sets are generated per fold

### optional external test datasets used in the test phase of training epochs
covers80:
  query_path: "data/covers80/full.txt"
  ref_path: "data/covers80/full.txt"
  every_n_epoch_to_test: 1  # validate after every n epoch
  
# Download the reels50easy dataset from https://www.irishtune.info/public/MLdata.htm
# and then you can uncomment the next 4 lines.
# reels50easy:
#  query_path: "data/reels50easy_testset/full.txt"  # path to your prepared data
#  ref_path: "data/reels50easy_testset/full.txt"
#  every_n_epoch_to_test: 1

# Download the reels50hard dataset from https://www.irishtune.info/public/MLdata.htm
# and then you can uncomment the next 4 lines.
#reels50hard:
#  query_path: "data/reels50hard_testset/full.txt"  # path to your prepared data
#  ref_path: "data/reels50hard_testset/full.txt"
#  every_n_epoch_to_test: 1

#shs_test:
#  query_path: "training/shs100k/test.txt"
#  ref_path: "training/shs100k/test.txt"
#  every_n_epoch_to_test: 2 # validate after every n epoch

#da-tacos: (see https://github.com/MTG/da-tacos)
#  query_path: "training/benchmark_query.txt"
#  ref_path: "training/benchmark_ref.txt"
#  query_in_ref_path: "training/datacos/query_in_ref.txt"
#  every_n_epoch_to_test: 2  # validate after every n epoch


every_n_epoch_to_test: 1  # validate after every n epoch
every_n_epoch_to_save: 1  # save model after every n epoch

### Dataset parameters
data_type: "cqt"  # raw or cqt or mel
chunk_frame: [1125, 900, 675]
chunk_s: 135  # = chunk_frame[0] / 25 * mean_size
mode: "random"
mean_size: 3
m_per_class: 8
cqt:
  hop_size: 0.04  # 1s has 25 frames

spec_augmentation:
  random_erase:
    prob: 0.5
    erase_num: 4
    region_size: [.25,.1]
  roll_pitch:
    prob: 0.5
    shift_num: 12
    method: "default" # "default" "low_melody" or "flex_melody"

### Training parameters
device: 'mps' # 'mps' or 'cuda' 
seed: 12345
num_workers: 1
num_gpus: 1
batch_size: 32  # 256. CoverHunter repo started with 16, but that is too small.
learning_rate: 0.001
adam_b1: 0.9
adam_b2: 0.999
lr_decay: 0.99  # CoverHunter used 0.9975 but that's for very large datasets
min_lr: 0.0001
warmup: False
warmup_steps: 20000
# hold_steps: Number of training steps to hold at the initial learning rate
# before exponential decay begins. Creates a "plateau" period for model
# stabilization.
#
# In train_prod, used only for the first fold. See fold_hold_steps below
#
# Set to 0 to disable (immediate decay). Typical values: 2000-5000 steps.
# Note: Mutually exclusive with warmup; if warmup=True, hold_steps is ignored.
hold_steps: 0

# --- Early Stopping Configuration ---
# early_stop_metric: "val_loss" or "mAP"
#   - "val_loss" (default): Stop when validation loss plateaus. All testsets
#     remain uncontaminated benchmarks suitable for publication.
#   - "mAP": Stop when smoothed mAP plateaus on map_stopping_testsets.
#     Checkpoint selection uses best raw mAP epoch.
#
# For research with publication-grade results:
#   Set early_stop_metric: "mAP" and list only validation-tier testsets in
#   map_stopping_testsets. Other testsets (defined above in this file)
#   will be evaluated and logged but never influence training decisions,
#   preserving their validity as held-out benchmarks.
early_stop_metric: "val_loss"
early_stopping_patience: 3

# map_stopping_testsets: List of testset names that drive stopping decisions.
#   Only used when early_stop_metric: "mAP"
#   Testsets not in this list are benchmark-only (logged but uncontaminated).
#
# map_stopping_testsets: ["yourInternalTestset"]

# map_smoothing_alpha: EMA weight for current mAP observation (0.0-1.0).
#   Higher values = more responsive to recent changes, more noise sensitivity.
#   Lower values = more stable, slower to detect genuine plateaus.
#   Only used when early_stop_metric: "mAP"
#
# map_smoothing_alpha: 0.3


### Training parameters only used by train_prod
k_folds: 5                   # number of folds for cross-validation
lr_initial: 0.0001           # starting learning rate for fold 2 (fold 1 uses learning_rate)
                             # so that each fold's initial learning decays toward min_lr
# fold_hold_steps: Used only for folds 2-K
#   to help folds adapt after loading checkpoints from previous folds,
#   where aggressive early decay can hurt convergence. 
fold_hold_steps: 2000
fold_lr_decay: 0.9925        # learning rate decay for folds 2-K
final_lr_decay: 0.9925       # learning rate decay for final full dataset training
full_dataset_lr_boost: 2     # add to min_lr this multiple of lr_step by this for the full dataset training run
final_early_stopping_patience: 10  # recommend longer patience for final full dataset training



### Model parameters
input_dim: 96 # default 96 frequency bins
embed_dim: 128
encoder:  # model-encode
  output_dims: 128
  num_blocks: 6
  attention_dim: 256

pool_type: "attention"

# loss parameters
foc:  # focal
  output_dims: 30000
  weight: 1.0
  gamma: 2

triplet:
  margin: 0.3
  weight: 0.1

center:
  weight: 0.0
